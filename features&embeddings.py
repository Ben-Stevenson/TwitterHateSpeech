# -*- coding: utf-8 -*-
"""Features&Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mAB5iKqQZvEbGcGH1-_1xI3_5-n_IG_Y
"""

!pip install textstat
# !pip install transformers

"""# **BERT EMBEDDINGS INTO CLASSIFIER**"""

import csv
import pandas as pd
read = pd.read_csv("labeled_data.csv")
train_df = pd.DataFrame({'alpha':['a']*read.shape[0],
                              'label':read['Label'],
                              'text':read["Text"].replace(r'\n',' ',regex=True)
                             })
train_df["text"] = train_df['text'].str.replace("amp","").str.replace('http\S+|www.\S+', '', case=False).str.replace(r'\d+','')
# train_df.loc[train_df.label == 1, 'label'] = 0
# train_df.loc[train_df.label == 2, 'label'] = 1

import re
!pip install ekphrasis
from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

text_processor = TextPreProcessor(
    # terms that will be normalized
    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
    #     'time', 'url', 'date', 'number'],
    # # terms that will be annotated
    # annotate={"hashtag", "allcaps", "elongated", "repeated",
    #     'emphasis', 'censored'},
    # fix_html=True, 
    # segmenter="twitter", 
    # corrector="twitter", 
    unpack_hashtags=True,  # perform word segmentation on hashtags
    # unpack_contractions=True,  # Unpack contractions (can't -> can not)
    # spell_correct_elong=False,  # spell correction for elongated words
    # tokenizer=SocialTokenizer(lowercase=True).tokenize,
    # dicts=[emoticons]
)
def userRemove(sentence):
  
  sentence = re.sub('@[^\s]+','',sentence)
  sentence = re.sub('rt',"",sentence)
  return(sentence)

def char_three(sentence):
  p = re.findall(r'((\w)\2{2,})', sentence)
  
 
  if bool(p):
    for i in range(len(p)):
      sentence = sentence.replace(p[i][0],p[i][1])
  punc = re.findall(r'(([!?.,])\2+)', sentence)
  if bool(punc):
    for i in range(len(punc)):
      sentence = sentence.replace(punc[i][0],punc[i][1])
  return(sentence)
def hashtagSort(sentence):
  sentence = text_processor.pre_process_doc(sentence)
  sentence = re.sub('<[^\s]+>',"",sentence)
  return(sentence)

def emojiRemove(sentence):
  sentence = re.sub('&#[^\s]+;',"",sentence)
  return(sentence)



def processing(train_df):
  train_df['text'] = train_df['text'].apply(lambda x: re.sub(r'[^\w\s\d+!,.?@#]','', x.lower(),  ))
  train_df['text'] = train_df['text'].str.replace('http\S+|www.\S+', '', case=False)
  train_df['text'] = [char_three(sentence) for sentence in train_df['text']] 
  train_df['text'] = [userRemove(sent) for sent in train_df['text']]
  train_df['text'] = train_df['text'].apply(lambda x: hashtagSort(x))
  train_df['text'] = train_df['text'].apply(lambda x: emojiRemove(x))
  return(train_df)

df = processing(train_df)

print(train_df['label'].value_counts())

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings
warnings.filterwarnings('ignore')

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))

max_len = 0
for i in tokenized.values:
    if len(i) > max_len:
        max_len = len(i)

padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])

print(max_len)

attention_mask = np.where(padded != 0, 1, 0)

print(attention_mask)

input_ids = torch.tensor(np.array(padded))
attention_mask = torch.tensor(attention_mask)
# split for memory issues
input_ids1 = input_ids[:8000]
input_ids2 = input_ids[8000:16000]
input_ids3 = input_ids[16000:]
attention_mask1 = attention_mask[:8000]
attention_mask2 = attention_mask[8000:16000]
attention_mask3 = attention_mask[16000:]

with torch.no_grad():
  last_hidden_states1 = model(input_ids1, attention_mask = attention_mask1)

with torch.no_grad():
  last_hidden_states2 = model(input_ids2, attention_mask = attention_mask2)

with torch.no_grad():
  last_hidden_states3 = model(input_ids3, attention_mask = attention_mask3)

last_hidden_states = torch.cat([last_hidden_states1[0],last_hidden_states2[0], last_hidden_states3[0]])
features = last_hidden_states[:,0,:].numpy()
labels = df['label']

train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.2)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
lr_clf = LogisticRegression(C=1.6,max_iter = 3500)
lr_clf.fit(train_features, train_labels)
svm_clf = LinearSVC(loss = "hinge")
svm_clf.fit(train_features, train_labels)
mlpNN = MLPClassifier()
mlpNN.fit(train_features, train_labels)

lr_pred = lr_clf.predict(test_features)
mlp_pred = mlpNN.predict(test_features)
svm_pred = svm_clf.predict(test_features)

"""BERT --> Logistic Regression Results"""

from sklearn.metrics import classification_report
from sklearn.metrics import balanced_accuracy_score
# print('accuracy %s' % accuracy_score(y_pred, Test_Y))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(test_labels,lr_pred)

print("BAC score of BERT embeddings and LR: ",balanced_accuracy_score(test_labels, lr_pred))

print(classification_report(test_labels, lr_pred))

ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: BERT Embeddings -> Logistic Regression")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""BERT --> Neural Network 
Results
"""

from sklearn.metrics import classification_report

# print('accuracy %s' % accuracy_score(y_pred, Test_Y))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(test_labels,mlp_pred)

print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(test_labels, mlp_pred))

print(classification_report(test_labels, mlp_pred))

ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: BERT Embedding -> Neural Network")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""BERT --> LinearSVC Results"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(test_labels,svm_pred)

from sklearn.metrics import balanced_accuracy_score
print("BAC score of BERT embeddings and svm: ",balanced_accuracy_score(test_labels, svm_pred))
# 0.901

from sklearn.metrics import classification_report
print(classification_report(test_labels,svm_pred))
ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: BERT Embedding -> SVM")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Neither"])
ax.xaxis.set_ticklabels(["Hate","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""**BERT Embeddings + Text + features**"""

!pip install textstat
!pip install transformers

import csv
import pandas as pd
read = pd.read_csv("labeled_data.csv")
train_df = pd.DataFrame({
                              'label':read['Label'],
                              'text':read["Text"].replace(r'\n',' ',regex=True)
                             })
train_df["text"] = train_df['text'].str.replace(r'[^\w\s\d+!,.?@]','').str.replace("amp","").str.replace("RT", "").str.replace('http\S+|www.\S+', '', case=False).str.replace("'","").str.replace(r'\d+','')

import re
def userRemove(sentence):
  
  sentence = re.sub('@[^\s]+','',sentence)
  sentence = re.sub('rt',"",sentence)
  return(sentence)

def char_three(sentence):
  p = re.findall(r'((\w)\2{2,})', sentence)
  
 
  if bool(p):
    for i in range(len(p)):
      sentence = sentence.replace(p[i][0],p[i][1])
  punc = re.findall(r'(([!?.,])\2+)', sentence)
  if bool(punc):
    for i in range(len(punc)):
      sentence = sentence.replace(punc[i][0],punc[i][1])
  return(sentence)

def processing(train_df):
  train_df['text'] = train_df['text'].apply(lambda x: re.sub(r'[^\w\s\d+!,.?@]','', x.lower(),  ))
  train_df['text'] = train_df['text'].str.replace('http\S+|www.\S+', '', case=False)
  train_df['text'] = [char_three(sentence) for sentence in train_df['text']] 
  train_df['text'] = [userRemove(sent) for sent in train_df['text']]
  # train_df['text'] = train_df['text'].apply(lambda x: hashtagSort(x))
  # train_df['text'] = train_df['text'].apply(lambda x: emojiRemove(x))
  return(train_df)
df = processing(train_df)
df = df[:2000]

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings
warnings.filterwarnings('ignore')

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

df['embed'] = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
max_len = 64
for i in df['embed'].values:
    if len(i) > max_len:
        max_len = len(i)

padded = np.array([i + [0]*(max_len-len(i)) for i in df['embed'].values])
attention_mask = np.where(padded != 0, 1, 0)



input_ids = torch.tensor(np.array(padded))
attention_mask = torch.tensor(attention_mask)
input_ids1 = input_ids[:1000]
# input_ids2 = input_ids[1000:2000]
attention_mask1 = attention_mask[:1000]
# attention_mask2 = attention_mask[1000:2000]
with torch.no_grad():
  last_hidden_states1 = model(input_ids1, attention_mask = attention_mask1)

# with torch.no_grad():
#   last_hidden_states2 = model(input_ids2, attention_mask = attention_mask2)

# last_hidden_states = torch.cat([last_hidden_states1,last_hidden_states2])
# features = last_hidden_states[:,0,:].numpy()
flatten_embed = lambda x: torch.flatten(x[0])
labels = df['label']
labels = labels[:2000]



# df['embed'] = [array for array in features]

len(df)

# feat_col = [c for c in df.columns.values if c  not in ['label'] and not in ['embed']]

train_features, test_features, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size = 0.2)

len(train_labels)

import collections

class TfidfEmbeddingVectorizer(object):
  def __init__(self, word2vec):
    self.word2vec = word2vec
    self.word2weight = None
    self.dim = 768

  def fit(self, X, y):
    tfidf = TfidfVectorizer(analyzer=lambda x: x)
    tfidf.fit(X)
    # if a word was never seen - it must be at least as infrequent
    # as any of the known words - so the default idf is the max of 
    # known idf's
    max_idf = max(tfidf.idf_)
    self.word2weight = collections.defaultdict(
        lambda: max_idf,
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

    return self

  def transform(self, X):
    return X[self.word2vec]

from sklearn.base import BaseEstimator, TransformerMixin
from collections import Counter
from sklearn.preprocessing import StandardScaler

class TextSelector(BaseEstimator, TransformerMixin):
    """
    Transformer to select a single column from the data frame to perform additional transformations on
    Use on text columns in the data
    """
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
      
        return self

    def transform(self, X):
        return X[self.key]
    
class NumberSelector(BaseEstimator, TransformerMixin):
    """
    Transformer to select a single column from the data frame to perform additional transformations on
    Use on numeric columns in the data
    """
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[[self.key]]

class ItemSelector(BaseEstimator, TransformerMixin):

    def __init__(self, column):
        self.column = column

    def fit(self, X, y=None, **fit_params):
        return self

    def transform(self, X):
        return (X[self.column])

from typing import Callable, List, Optional, Tuple

import pandas as pd
from sklearn.base import TransformerMixin, BaseEstimator
import torch


class BertTransformer(BaseEstimator, TransformerMixin):
    def __init__(
            self,
            bert_tokenizer,
            bert_model,
            max_length: int = 57,
            embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,
    ):
        self.tokenizer = bert_tokenizer
        self.model = bert_model
        self.model.eval()
        self.max_length = max_length
        self.embedding_func = embedding_func

        if self.embedding_func is None:
            self.embedding_func = lambda x: x[0][:, 0, :]

    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:
        # Tokenize the text with the provided tokenizer
        tokenized_text = self.tokenizer.encode_plus(text,
                                                    add_special_tokens=True,
                                                    max_length=self.max_length
                                                    )["input_ids"]

        # Create an attention mask telling BERT to use all words
        attention_mask = [1] * len(tokenized_text)

        # bert takes in a batch so we need to unsqueeze the rows
        return (
            torch.tensor(tokenized_text).unsqueeze(0),
            torch.tensor(attention_mask).unsqueeze(0),
        )

    def _tokenize_and_predict(self, text: str) -> torch.tensor:
        tokenized, attention_mask = self._tokenize(text)

        embeddings = self.model(tokenized, attention_mask)
       
        return self.embedding_func(embeddings)

    def transform(self, text: List[str]):
        if isinstance(text, pd.Series):
          print(text)
          text = text.tolist()
            

        with torch.no_grad():
            return torch.stack([self._tokenize_and_predict(string) for string in text])

    def fit(self, X, y=None):
        """No fitting necessary so we just return ourselves"""
        return self

bert_transformer = BertTransformer(tokenizer, model)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from gensim.sklearn_api import W2VTransformer



text = Pipeline([
                
                ('count', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
])

embed = Pipeline([
                  ('transformer', bert_transformer),
                
])

from sklearn.pipeline import FeatureUnion
feats = FeatureUnion([
                      ('text', text),
                      ('embed', embed)
])
# feature_processing = Pipeline([('feats', feats)])

# feature_processing.fit_transform(train_features)



from sklearn.linear_model import LogisticRegression
pipeline = Pipeline([
                     ('feats', feats),
                     ('clf', LogisticRegression()) # classifier
])
# pipeline
# pipeline.fit_transform(train_text)
pipeline.fit(train_features, train_labels)
preds_lr = pipeline.predict(test_features)

from sklearn.metrics import classification_report
from sklearn.metrics import balanced_accuracy_score
# print('accuracy %s' % accuracy_score(y_pred, Test_Y))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(test_labels,preds_lr)

print("BAC score of BERT embeddings and LR: ",balanced_accuracy_score(test_labels, preds_lr))

print(classification_report(test_labels, preds_lr))

ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: BERT Embeddings -> Logistic Regression")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""# **FEATURE UNION**"""

import nltk
from nltk import word_tokenize, FreqDist
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = stopwords.words('english')
from nltk.tokenize import TweetTokenizer
from nltk import ngrams
tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)
from nltk.tokenize import sent_tokenize
import pandas as pd
nltk.download('averaged_perceptron_tagger')

WNlemma = nltk.WordNetLemmatizer()
import re
import numpy as np

read = pd.read_csv("labeled_data.csv")
df = pd.DataFrame({'label':read['Label'],
                              'text':read["Text"].replace(r'\n',' ',regex=True)
                             })
df["text"] = df['text'].str.replace("amp","").str.replace("'","")

# df = df.drop(6098)
df['text'][6098]

!pip install ekphrasis
# !pip install textstat
from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons

text_processor = TextPreProcessor(
    # terms that will be normalized
    # normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
    #     'time', 'url', 'date', 'number'],
    # # terms that will be annotated
    # annotate={"hashtag", "allcaps", "elongated", "repeated",
    #     'emphasis', 'censored'},
    # fix_html=True, 
    # segmenter="twitter", 
    # corrector="twitter", 
    unpack_hashtags=True,  # perform word segmentation on hashtags
    # unpack_contractions=True,  # Unpack contractions (can't -> can not)
    # spell_correct_elong=False,  # spell correction for elongated words
    # tokenizer=SocialTokenizer(lowercase=True).tokenize,
    # dicts=[emoticons]
)

def userRemove(sentence):
  if '@hoes' in sentence:
    sentence = re.sub('@',"",sentence)
  else:
    sentence = re.sub('@[^\s]+','',sentence)
    sentence = re.sub('rt',"",sentence)
  
  return(sentence)

def char_three(sentence):
  p = re.findall(r'((\w)\2{2,})', sentence)
  
 
  if bool(p):
    for i in range(len(p)):
      sentence = sentence.replace(p[i][0],p[i][1])
  punc = re.findall(r'(([!?.,])\2+)', sentence)
  if bool(punc):
    for i in range(len(punc)):
      sentence = sentence.replace(punc[i][0],punc[i][1])
  return(sentence)

FIRST_SINGULAR = ["i", "my", "mine", "myself"]
FIRST_PLURAL = ["we", "our", "ours", "ourselves"]
SECOND_SINGULAR = ["you", "your", "yours", "yourself"]
SECOND_PLURAL = ["you", "your", "yours", "yourselves"]
THIRD_SINGULAR_MASCULINE = ["he", "his", "him", "himself"]
THIRD_SINGULAR_FEMININE = ["she", "her", "hers", "herself"]
THIRD_PLURAL = ["they", "their", "theirs", "themselves"]
FIRST_SECOND_PERSON = set(FIRST_SINGULAR + SECOND_PLURAL + SECOND_SINGULAR + SECOND_PLURAL)
THIRD_PERSON = set(THIRD_SINGULAR_FEMININE + THIRD_SINGULAR_MASCULINE + THIRD_PLURAL)
def count_pronouns(doc):
    segment = doc.split()
    counter = {"1sg": 0, "1pl": 0, "2": 0, "3": 0}
    for pronoun in FIRST_SINGULAR:
        counter["1sg"] += segment.count(pronoun)
    for pronoun in FIRST_PLURAL:
        counter["1pl"] += segment.count(pronoun)
    for pronoun in set(SECOND_PLURAL + SECOND_SINGULAR):
      counter["2"] += segment.count(pronoun)
    for pronoun in THIRD_PERSON:
        counter["3"] += segment.count(pronoun)
    return sum(counter.values())

def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

sentence = "you have to be joking you have go you"
def pronoun_verb(sentence):
  sentence = tknzr.tokenize(sentence)
  sentence = nltk.pos_tag(sentence)
  sentence = [words[1] for words in sentence]
  i = 0
  count = 0
  while i < len(sentence) - 1:
    
    if sentence[i].startswith("PR"):
      if sentence[i+1].startswith("VB") or sentence[i+1].startswith("MD"):
        count = count + 1
      i = i + 2
    else:
      i = i + 1
  
  return count
def hashtagSort(sentence):
  sentence = text_processor.pre_process_doc(sentence)
  sentence = re.sub('<[^\s]+>',"",sentence)
  return(sentence)

def emojiRemove(sentence):
  sentence = re.sub('&#[^\s]+;',"",sentence)
  return(sentence)

def processing(train_df):
  train_df['text'] = [char_three(sentence) for sentence in train_df['text']] 
  train_df['text'] = [userRemove(sent) for sent in train_df['text']]
  train_df['text'] = train_df['text'].apply(lambda x : emojiRemove(x))
  train_df['text'] = train_df['text'].apply(lambda x : hashtagSort(x))
  train_df['text'] = train_df['text'].apply(lambda x: re.sub(r'[^\w\s\d+!,.?@]','', x.lower(),  ))
  train_df['text'] = train_df['text'].str.replace('http\S+|www.\S+', '', case=False)
  
  
  return(train_df)

def dataStrip(sentence):
  #uses regex sub function to remove username e.g. @username
  # and retweet metadata 
  #exception made for one instance @hoes which simple removes @
  if '@hoes' in sentence:
    sentence = re.sub('@',"",sentence)
  else:
    sentence = re.sub('@[^\s]+','',sentence)
    sentence = re.sub('rt',"",sentence)
  #uses regex findall function to search for occurences of string or 
  #punctuation that occurs more than 2 times in a row
  p = re.findall(r'((\w)\2{2,})', sentence)
  if bool(p):
    for i in range(len(p)):
      sentence = sentence.replace(p[i][0],p[i][1])
  punc = re.findall(r'(([!?.,])\2+)', sentence)
  if bool(punc):
    for i in range(len(punc)):
      sentence = sentence.replace(punc[i][0],punc[i][1])

  #using ekphrasis library all hashtags are split into word pieces. e.g. #Donthurtme --> Dont hurt me
  sentence = text_processor.pre_process_doc(sentence)
  
  #Removes instances of HTML code 
  sentence = re.sub('&#[^\s]+;',"",sentence)
  return(sentence)

df = processing(df)

df['text'][6098]

def features(train_df):

  train_df['pos_tag'] = [tknzr.tokenize(lines) for lines in train_df['text']]
  train_df['pos_tag'] = [nltk.pos_tag(words) for words in train_df['pos_tag']]
  train_df['pos_tag'] = [[words[1] for words in tuples] for tuples in train_df['pos_tag']]
  train_df['pos_tag'] = [" ".join(words) for words in train_df['pos_tag']]
  train_df['unigram'] = [[item for item in ngrams(sent.split(),n)] for n in range(1,2) for sent in train_df['text']]
  train_df['unigram'] = [" ".join([" ".join(unigram) for unigram in items]) for items in train_df['unigram']]
  train_df['bigram'] = [[item for item in ngrams(sent.split(),n)] for n in range(2,3) for sent in train_df['text']]
  train_df['bigram'] = [" ".join([" ".join(bigram) for bigram in items]) for items in train_df['bigram']]
  train_df['trigram'] = [[item for item in ngrams(sent.split(),n)] for n in range(3,4) for sent in train_df['text']]
  train_df['trigram'] = [" ".join([" ".join(bigram) for bigram in items]) for items in train_df['trigram']]
  train_df['word_count'] = train_df['text'].apply(lambda x : len(x.split()))
  
  
  train_df['num_unique_words'] = train_df['text'].apply(lambda x: len(set(w for w in x.split())))
  train_df['words_vs_unique'] = df['num_unique_words'] / df['word_count']
  train_df['words_vs_unique'] = train_df['words_vs_unique'].apply(lambda x: round(x, 2))
  train_df['pronoun_count'] = train_df['text'].apply(lambda x: count_pronouns(x))
  train_df['pronoun_verb'] = train_df['text'].apply(lambda x: pronoun_verb(x))
  train_df['avg_word_length'] = train_df['text'].apply(lambda x: np.mean([len(t) for t in x.split(' ')]))
  train_df['avg_sent_length'] = train_df['text'].apply(lambda x: np.mean([len(t.split()) for t in x.split(".")]))
  train_df['num_sent'] = train_df['text'].apply(lambda x: (len([len(t) for t in x.split(".")])))
 
  return(train_df)
df = features(df)
import seaborn as sns
from textstat import *


READABILITY_FEATURES = {'n_difficult_words':difficult_words,
 'flesch_kincaid_grade':flesch_kincaid_grade,
 'flesch_reading_ease':flesch_reading_ease,
 }
 
READABILITY_FEATURES1 = ['n_difficult_words',
 'flesch_kincaid_grade',
 'flesch_reading_ease',

]

df1 = pd.DataFrame(columns = READABILITY_FEATURES1)
df = pd.concat((df, df1), ignore_index=True)
x = ['label','text', 'pos_tag','avg_sent_length','pronoun_count', 'pronoun_verb','word_count','unigram','trigram','bigram','words_vs_unique','avg_word_length']+READABILITY_FEATURES1
df = df[x]


for stat, func in READABILITY_FEATURES.items():
  df[stat] = [func(line) for line in df['text']]



filename = "lexiconslist"
with open(filename) as f:
    content = f.readlines()
# you may also want to remove whitespace characters like `\n` at the end of each line
hate_terms = [x.strip() for x in content] 

def splitTextToTuple(string, n):
    words = string.split()
    grouped_words = [' '.join(words[i: i + n]) for i in range(0, len(words), n)]
    return grouped_words

hate_uni = [words for words in hate_terms if len(words.split()) == 1]
hate_bi = [words for words in hate_terms if len(words.split()) == 2]
hate_tri = [words for words in hate_terms if len(words.split()) == 3]


import inspect
def var_to_string(var):
  for fi in reversed(inspect.stack()):
            names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]
            if len(names) > 0:
                return names[0]

def count_hate(column, n, hate_ngram, dataframe):
  column_name = "hate_" + var_to_string(hate_ngram).split("_")[1]
  dataframe[column_name] = [splitTextToTuple(sentence, n) for sentence in dataframe[column]]
  
  
  index = 0
  
  
  for lists in dataframe[column_name]:
    count = 0
    for ngram in lists:
      if ngram in hate_ngram:
        count+=1
        dataframe.at[index, column_name] = count
    index+=1

  x = [items for items in dataframe[column_name] if isinstance(items, list)==False]
  index = 0
  for rows in dataframe[column_name]:
    if rows not in x:
      dataframe.at[index, column_name] = 0
    index+=1
  
  return(dataframe)

#Add new columns to dataset
df = count_hate('unigram', 1, hate_uni, df)
df = count_hate('bigram', 2, hate_bi, df)
df = count_hate('trigram', 3, hate_tri, df)

df.columns

np.mean([df['hate_tri'][df['label'] == 0]])

# import matplotlib.pyplot as plt
# import seaborn as sns
# print(df.columns)

# filtered_class = df
# sns.lmplot("flesch_reading_ease", "hate_uni", data=filtered_class, hue="label", fit_reg=False)

features= [c for c in df.columns.values if c  not in ['label']]
numeric_features= [c for c in df.columns.values if c  not in ['text','label']]
target = 'label'
from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.3)



from sklearn.base import BaseEstimator, TransformerMixin
from collections import Counter
from sklearn.preprocessing import StandardScaler

class TextSelector(BaseEstimator, TransformerMixin):
    """
    Transformer to select a single column from the data frame to perform additional transformations on
    Use on text columns in the data
    """
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
      
        return self

    def transform(self, X):
        return X[self.key]
    
class NumberSelector(BaseEstimator, TransformerMixin):
    """
    Transformer to select a single column from the data frame to perform additional transformations on
    Use on numeric columns in the data
    """
    def __init__(self, key):
        self.key = key

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[[self.key]]

    
class PosTagMatrix(BaseEstimator, TransformerMixin):
    #normalise = True - devide all values by a total number of tags in the sentence
    #tokenizer - take a custom tokenizer function
    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):
        self.tokenizer=tokenizer
        self.normalize=normalize

    #helper function to tokenize and count parts of speech
    def pos_func(self, sentence):
        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))

    # fit() doesn't do anything, this is a transformer class
    def fit(self, X, y = None):
        return self

    #all the work is done here
    def transform(self, X):

        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)
        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)
        if self.normalize:
            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)
        
        return X_tagged

class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):
    """Takes in dataframe, extracts road name column, outputs average word length"""

    def __init__(self):
        pass

    def average_word_length(self, name):
        """Helper code to compute average word length of a name"""
        return np.mean([len(word) for word in name.split()])

    def transform(self, df, y=None):
        """The workhorse of this feature extractor"""
        return df['label_id'].apply(self.average_word_length)

    def fit(self, df, y=None):
        """Returns `self` unless something different happens in train and test"""
        return self

# X_train.isnull().any()
# X_train[np.isnan(X_train['words_vs_unique'])]
# # np.all(np.isfinite(X_train['words_vs_unique']))

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

data = Pipeline([
                ('selector', TextSelector(key='text')),
                ('count', TfidfVectorizer(ngram_range = (1,2), stop_words = 'english', min_df = 3, max_df=0.85))
              ])



pos_tagger = Pipeline([
                    ('selector', TextSelector(key='pos_tag')),
                    ('tfidf', TfidfVectorizer()),
                    # ('pos', PosTagMatrix(tokenizer=tknzr.tokenize,normalize=False)),
])


# uni_gram = Pipeline([
#                      ('selector',TextSelector(key='unigram')),
#                      ('tfidf',TfidfVectorizer(ngram_range=(1))),
# ])

bi_gram = Pipeline([
                     ('selector',TextSelector(key='bigram')),
                     ('tfidf',TfidfVectorizer(ngram_range=(1,2))),
])

# length =  Pipeline([
#                 ('selector', NumberSelector(key='length')),
#                 ('standard', StandardScaler())
#             ])
pronoun_count = Pipeline([
                  ('selector', NumberSelector(key = 'pronoun_count')),
                  ('standard', StandardScaler())
])

pronoun_verb = Pipeline([
                  ('selector', NumberSelector(key = 'pronoun_verb')),
                  ('standard', StandardScaler())
])
words =  Pipeline([
                ('selector', NumberSelector(key='word_count')),
                ('standard', StandardScaler())
            ])

words_vs_unique =  Pipeline([
                ('selector', NumberSelector(key='words_vs_unique')),
                ('standard', StandardScaler())
            ])
avg_word_length =  Pipeline([
                ('selector', NumberSelector(key='avg_word_length')),
                ('standard', StandardScaler())
            ])
avg_sent_length =  Pipeline([
                ('selector', NumberSelector(key='avg_sent_length')),
                ('standard', StandardScaler()),
            ])
num_sent =  Pipeline([
                ('selector', NumberSelector(key='num_sent')),
                ('standard', StandardScaler()),
            ])
# max_sent_length =  Pipeline([
#                 ('selector', NumberSelector(key='max_sent_length')),
#                 ('standard', StandardScaler()),
#             ])
# max_sent_length =  Pipeline([
#                 ('selector', NumberSelector(key='')),
#                 ('standard', StandardScaler()),
#             ])
uni_hate = Pipeline([
                     ('selector', NumberSelector(key="hate_uni")),
                     ('standard', StandardScaler())
])
bi_hate = Pipeline([
                    ('selector', NumberSelector(key="hate_bi")),
                    ('standard', StandardScaler())
])
tri_hate = Pipeline([
                     ('selector', NumberSelector(key="hate_tri")),
                     ('standard', StandardScaler())
])
difficult_words = Pipeline([
                     ('selector', NumberSelector(key="n_difficult_words")),
                     ('standard', StandardScaler())
])
readability_index = Pipeline([
                     ('selector', NumberSelector(key="automated_readability_index")),
                     ('standard', StandardScaler())
])

flesch_kincaid = Pipeline([
                     ('selector', NumberSelector(key="flesch_kincaid_grade")),
                     ('standard', StandardScaler())
])
flesch_read = Pipeline([
                     ('selector', NumberSelector(key="flesch_reading_ease")),
                     ('standard', StandardScaler())
])

from sklearn.pipeline import FeatureUnion
feats = FeatureUnion([('data', data),
                      ('pos_tagger', pos_tagger), 
                      # ('unigram',uni_gram),
                      ('bigram', bi_gram),
                      # # ('trigram', tri_gram),
                      ('words', words),
                      ('avg_word_length', avg_word_length),
                      ('avg_sent_length', avg_sent_length),
                      # ('pronoun_count', pronoun_count),
                      ('pronoun_verb', pronoun_verb),
                      ('words_vs_unique', words_vs_unique),
                      ('uni_hate', uni_hate),
                      ('bi_hate', bi_hate),
                      ('tri_hate', tri_hate),
                      ('dw', difficult_words),
                      ('fkg',flesch_kincaid),
                      ('fre',flesch_read),
                     
                      
                      ])

feature_processing = Pipeline([('feats', feats)])

feature_processing.fit_transform(X_train)

"""**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LogisticRegression

lr_pipe1 = Pipeline([
    ('features',feats),
    
    ('clf',LogisticRegression(penalty='none', max_iter = 4500))
])

lr_pipe1.fit(X_train, y_train)

preds_lr = lr_pipe1.predict(X_test)

from sklearn.metrics import balanced_accuracy_score
print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(y_test, preds_lr))

from sklearn.metrics import classification_report
print(classification_report(y_test, preds_lr))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(y_test,preds_lr)






ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: HCF Log Reg")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

# count = 0
# pred_list = []
# for idx, prediction, label in zip(enumerate(X_test['text']), preds_lr,  y_test):
#     if prediction != label:
#         if label == 0 and prediction == 1:
#             count= count + 1
#             pred_list.append(idx[1])
#             # print("Sample", idx, ', has been classified as', prediction, 'and should be', label, "\n")
# # print(count)
import statsmodels.api as sm

"""**SVM**"""

from sklearn.svm import LinearSVC

svm_pipe1 = Pipeline([
    ('features',feats),
    
    ('clf',LinearSVC(max_iter = 4500, loss = "hinge"))
])

svm_pipe1.fit(X_train, y_train)

preds_svm = svm_pipe1.predict(X_test)

from sklearn.metrics import balanced_accuracy_score
print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(y_test, preds_svm))

from sklearn.metrics import classification_report
print(classification_report(y_test, preds_svm))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(y_test,preds_svm)






ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: HCF LinearSVC")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

import pickle
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from pprint import pprint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import ShuffleSplit
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

tfidf = TfidfVectorizer(encoding='utf-8',
                        ngram_range=(1,3),
                        stop_words=None,
                        lowercase=False,
                        
                        # max_features=max_features,
                        norm='l2',
                        sublinear_tf=True)
                        
features_train = tfidf.fit_transform(X_train['text']).toarray()

labels_train = y_train
print(features_train.shape)

features_test = tfidf.transform(X_test['text']).toarray()
labels_test = y_test
print(features_test.shape)

svc_0 = LinearSVC(random_state=8)

print('Parameters currently in use:\n')
pprint(svc_0.get_params())

# C
C = [.0001, .001, .01, 1]

# gamma
loss = ["squared_hinge", "hinge"]

# degree
tol = [1e-5, 1e-4, 1e-3, 1e-6]

# Create the random grid
random_grid = {'C': C,
              'loss':loss,
               'tol':tol
             }

pprint(random_grid)

svc = svm.LinearSVC(random_state=8)

# Definition of the random search
random_search = RandomizedSearchCV(estimator=svc,
                                   param_distributions=random_grid,
                                   n_iter=50,
                                   scoring='accuracy',
                                   cv=3, 
                                   verbose=1, 
                                   random_state=8)

# Fit the random search model
random_search.fit(features_train, y_train)







"""**NEURAL NETWORK**"""

from sklearn.neural_network import MLPClassifier

nn_pipe1 = Pipeline([
    ('features',feats),
    
    ('clf',MLPClassifier(solver = 'adam'))
])

nn_pipe1.fit(X_train, y_train)

preds_nn = nn_pipe1.predict(X_test)

from sklearn.metrics import balanced_accuracy_score
print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(y_test, preds_nn))

from sklearn.metrics import classification_report
print(classification_report(y_test, preds_nn))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(y_test,preds_nn)






ax = plt.subplot()


plt.title("Confusion Matrix of the classifier: HCF and NN")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()



"""# **BINARY** **CLASSIFICATION**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
read_conf = pd.read_csv("labeled_data.csv")
df = pd.DataFrame({'label':read_conf['Label'],
                              'text':read_conf["Text"].replace(r'\n',' ',regex=True)
                             })
df["text"] = df['text'].str.replace("amp","").str.replace('http\S+|www.\S+', '', case=False).str.replace(r'\d+','')

df['text'][16740]

# df['text'][df['text'].str.contains("beaner") == True]

import re
def userRemove(sentence):
  
  sentence = re.sub('@[^\s]+','',sentence)
  sentence = re.sub('rt',"",sentence)
  return(sentence)
def hashtagSort(sentence):
  sentence = text_processor.pre_process_doc(sentence)
  sentence = re.sub('<[^\s]+>',"",sentence)
  return(sentence)

def emojiRemove(sentence):
  sentence = re.sub('&#[^\s]+;',"",sentence)
  sentence = re.sub('niggah',"nigger", sentence)
  return(sentence)
df['text'] = df['text'].apply(lambda x : x.lower())
df['text'] = df['text'].apply(lambda x : emojiRemove(x))
df['text'] = df['text'].apply(lambda x : hashtagSort(x))
df['text'] = [userRemove(text) for text in df['text']]

# df.loc[df.label == 1, 'label'] = 0
# df.loc[df.label == 2, 'label'] = 1

Train_X, Test_X, Train_Y, Test_Y = train_test_split(df['text'],df['label'],test_size=0.2)

Test_X[Test_X.str.contains("what's this chinks" ) == True]



len(Test_X)

from sklearn.pipeline import Pipeline

from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

svm_pipe = Pipeline([('vect', TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.95,ngram_range = (1,2))),
                     ('clf', LinearSVC( loss='hinge')),
              ])
# param_grid = {'clf__C':[0.01,0.1,1,10,100,1000],'clf__multi_class':['ovr', 'crammer_singer'], 'clf__penalty':['l1','l2']}
# grid = GridSearchCV(svm_pipe,param_grid,refit = True, verbose=2)
svm_pipe.fit(Train_X, Train_Y)
# grid.fit(Train_X,Train_Y)



nn_pipe = Pipeline([('vect', TfidfVectorizer()),
               ('clf', MLPClassifier(solver = 'adam')),
              ])
nn_pipe.fit(Train_X, Train_Y)


# nb_pipe = Pipeline([('vect', TfidfVectorizer(stop_words = 'english',ngram_range=(1,3))),
#                ('clf', MultinomialNB(,) ),
#               ])
# nb_pipe.fit(Train_X, Train_Y)

svm_pred = svm_pipe.predict(Test_X)

nn_pred = nn_pipe.predict(Test_X)
# nb_pred = nb_pipe.predict(Test_X)

"""Logistic Regression Binary"""



from sklearn.linear_model import LogisticRegression



lr_pipe = Pipeline([('vect', TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.85,ngram_range = (1,2))),
               ('clf', LogisticRegression(C=1.6,max_iter = 4000)),
              ])
# param_grid = {'clf__penalty' : ['l1', 'l2'],'clf__C' : np.logspace(-4, 4, 20),'clf__solver' : ['liblinear']}
# grid = GridSearchCV(lr_pipe,param_grid,refit = True, verbose=2)
# grid.fit(Train_X,Train_Y)
# print(grid.best_params_)

lr_pipe.fit(Train_X, Train_Y)

lr_pred = lr_pipe.predict(Test_X)
from sklearn.metrics import classification_report
import numpy as np
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import f1_score
# print('accuracy %s' % accuracy_score(y_pred, Test_Y))
print(classification_report(Test_Y, lr_pred))
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(Test_Y,lr_pred)


print("BAC score of BERT embeddings and LR: ",balanced_accuracy_score(Test_Y, lr_pred))

print(f1_score(Test_Y, lr_pred, average='micro'))

ax = plt.subplot()


plt.title("Confusion Matrix of the LR classifier: Log Reg")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(conf_mat,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate","Offensive", "Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""Support Vector Machine Binary"""

from sklearn.metrics import classification_report
from sklearn.metrics import balanced_accuracy_score

# print('accuracy %s' % accuracy_score(y_pred, Test_Y))
print(classification_report(Test_Y, svm_pred))
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(Test_Y,svm_pred)


print("BAC score of BERT embeddings and SVM: ",balanced_accuracy_score(Test_Y, svm_pred))

print(f1_score(Test_Y, svm_pred, average='micro'))


ax = plt.subplot()


plt.title("Confusion Matrix of the SVC classifier: LinearSVC")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(conf_mat,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate" ,"Offensive", "Neither"])
ax.xaxis.set_ticklabels(["Hate","Offensive","Neither"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

count = 0
for idx, prediction, label in zip(enumerate(Test_X), lr_pred,  Test_Y):
    if prediction != label:
        if label == 0 and prediction == 2:
            count= count + 1
            print("Sample", idx, ', has been classified as', prediction, 'and should be', label, "\n")
print(count)

print(Test_X[Test_X.str.contains('fuck you nigg') == True])

"""Neural Network Binary"""

from sklearn.metrics import classification_report


# print('accuracy %s' % accuracy_score(y_pred, Test_Y))
print(classification_report(Test_Y, nn_pred))
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
conf_mat = confusion_matrix(Test_Y,nn_pred)


print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(Test_Y, nn_pred))

print(f1_score(Test_Y, nn_pred, average='micro'))

ax = plt.subplot()


plt.title("Confusion Matrix of the MLP classifier: NN")
cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
ax.yaxis.set_ticklabels(["Hate + Offensive", "n"])
ax.xaxis.set_ticklabels(["Hate + Offensive","n"])
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
plt.show()
plt.draw()

"""Naive Bayes"""

# from sklearn.metrics import classification_report
# import numpy as np

# # print('accuracy %s' % accuracy_score(y_pred, Test_Y))
# print(classification_report(Test_Y, nb_pred))
# from sklearn.metrics import confusion_matrix
# import matplotlib.pyplot as plt
# import seaborn as sns
# conf_mat = confusion_matrix(Test_Y,nb_pred)

# from sklearn.metrics import balanced_accuracy_score
# print("BAC score of BERT embeddings and NN: ",balanced_accuracy_score(Test_Y, nb_pred))



# ax = plt.subplot()


# plt.title("Confusion Matrix of the MLP classifier: Conflated Hate and Offensive")
# cmn = conf_mat.astype('float')/conf_mat.sum(axis=1)[:,np.newaxis]
# sns.heatmap(cmn,annot=True, ax = ax, fmt='.3f',cmap='Blues')
# ax.yaxis.set_ticklabels(["Hate","O","N"])
# ax.xaxis.set_ticklabels(["Hate","O","N"])
# ax.set_xlabel('Predicted')
# ax.set_ylabel('Actual')
# plt.show()
# plt.draw()

"""**Lime analysis**"""

!pip install lime

import sklearn.metrics
from __future__ import print_function
lr_pipe.predict_proba

from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=['Hate',"Offensive","Neither"])

print(Test_Y[Test_Y == 0])

exp = explainer.explain_instance(Test_X[2145], lr_pipe.predict_proba, labels=[0,1,2])

from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=['Hate',"Offensive","Neither"])
exp = explainer.explain_instance(Test_X[7372], lr_pipe.predict_proba, labels=[0,1,2])
print ('Explanation for class: Hate')
print ('\n'.join(map(str, exp.as_list(label=0))))
print ()
print ('Explanation for class: Offensive')
print ('\n'.join(map(str, exp.as_list(label=1))))
print()
print ('Explanation for class: Neither')
print ('\n'.join(map(str, exp.as_list(label=2))))
print ()
exp.show_in_notebook(text=False)

exp.show_in_notebook(text=False)

